{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_paper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tinynja/Sarsa-phi-EB/blob/main/rl_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0ztbgbOZ-uJ",
        "outputId": "3de73969-cf5b-4193-b1a6-4fbe83136c29"
      },
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !rm -rf *\n",
        "    !git clone https://github.com/Tinynja/Sarsa-phi-EB\n",
        "    !mv Sarsa-phi-EB/* .\n",
        "    !rm -rf Sarsa-phi-EB\n",
        "    # DON'T install packages defined in Pipfile_colab_remove\n",
        "    !sed -ri \"/$(tr '\\n' '|' < Pipfile_Colab_exclude)/d\" Pipfile\n",
        "else:\n",
        "    print('Skipping GitHub cloning since not running in Colab.')\n",
        "    \n",
        "# Install required dependencies\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Colab doesn't support pipenv, hence we convert Pipfile into requirements.txt\n",
        "    if 'requirements_Colab.txt' not in os.listdir():\n",
        "        !pip install pipenv\n",
        "        !pipenv lock -r > requirements.txt\n",
        "    !pip install -r requirements_Colab.txt 1> /dev/null\n",
        "else:\n",
        "    !pipenv install 1> /dev/null\n",
        "\n",
        "# Import all supported ROMs into ALE\n",
        "!ale-import-roms ROMS\n",
        "#### ALE-related imports ####\n",
        "import torch\n",
        "\n",
        "# Built-in libraries\n",
        "import re\n",
        "import sys\n",
        "import base64\n",
        "import pickle\n",
        "import random\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Pypi libraries\n",
        "import numpy as np\n",
        "from IPython import display as ipythondisplay\n",
        "from ale_py import ALEInterface, SDL_SUPPORT\n",
        "import ale_py.roms as ROMS\n",
        "\n",
        "\n",
        "!pip install matplotlib numpy gym\n",
        "\n",
        "# env\n",
        "import gym\n",
        "\n",
        "# data manipulation, colab dispaly, and plotting\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# misc util\n",
        "import random, glob, base64, itertools\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "# Configuration\n",
        "device = 'cuda' if torch.cuda.device_count() else 'cpu'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sarsa-phi-EB'...\n",
            "remote: Enumerating objects: 281, done.\u001b[K\n",
            "remote: Counting objects: 100% (281/281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (233/233), done.\u001b[K\n",
            "remote: Total 281 (delta 79), reused 199 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (281/281), 710.79 KiB | 2.78 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n",
            "/bin/bash: Pipfile_colab_remove: No such file or directory\n",
            "sed: -e expression #1, char 0: no previous regular expression\n",
            "Pipfile.lock not found, creating...\n",
            "Locking [dev-packages] dependencies...\n",
            "Locking [packages] dependencies...\n",
            "\u001b[KBuilding requirements...\n",
            "\u001b[KResolving dependencies...\n",
            "\u001b[K\u001b[?25h\u001b[32m\u001b[22m✔ Success!\u001b[39m\u001b[22m\u001b[0m \n",
            "Updated Pipfile.lock (03135b)!\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            ms_pacman    ROMS/Ms. Pac-Man (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               amidar         ROMS/Amidar (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        haunted_house ROMS/Haunted House (Mystery Mansion, Graves' Manor, Nightmare Manor) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               gopher ROMS/Gopher (Gopher Attack) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            centipede      ROMS/Centipede (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           beam_rider      ROMS/Beamrider (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         demon_attack ROMS/Demon Attack (Death from Above) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          road_runner    ROMS/Road Runner (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              berzerk        ROMS/Berzerk (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           mario_bros    ROMS/Mario Bros. (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                   et ROMS/E.T. - The Extra-Terrestrial (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           time_pilot     ROMS/Time Pilot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       journey_escape ROMS/Journey Escape (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 hero       ROMS/H.E.R.O. (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              koolaid ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             robotank ROMS/Robot Tank (Robotank) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Fun with Numbers (AKA Basic Math) (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            up_n_down     ROMS/Up 'n Down (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                mr_do        ROMS/Mr. Do! (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         flag_capture ROMS/Flag Capture - Capture (Capture the Flag) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            videocube ROMS/Atari Video Cube (Atari Cube, Video Cube) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             defender       ROMS/Defender (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             galaxian       ROMS/Galaxian (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              phoenix        ROMS/Phoenix (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            blackjack ROMS/Blackjack - Black Jack (Gambling) (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          laser_gates ROMS/Laser Gates (AKA Innerspace) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            asteroids      ROMS/Asteroids (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            space_war ROMS/Space War - Space Star (32 in 1) (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          battle_zone     ROMS/Battlezone (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            jamesbond ROMS/James Bond 007 (James Bond Agent 007) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                alien          ROMS/Alien (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Pinball (AKA Video Pinball) (Zellers).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            riverraid     ROMS/River Raid (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              hangman ROMS/Hangman - Spelling (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m    montezuma_revenge ROMS/Montezuma's Revenge - Featuring Panama Joe (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 klax           ROMS/Klax (1991).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              pitfall ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              solaris ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              frogger        ROMS/Frogger (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Video Pinball - Arcade Pinball (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              freeway        ROMS/Freeway (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             entombed ROMS/Entombed (Maze Chase, Pharaoh's Tomb, Zombie) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             gravitar       ROMS/Gravitar (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              assault ROMS/Assault (AKA Sky Alien) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             kangaroo       ROMS/Kangaroo (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m     human_cannonball ROMS/Human Cannonball - Cannon Man (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert        ROMS/Q. Bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                krull          ROMS/Krull (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      chopper_command ROMS/Chopper Command (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            frostbite      ROMS/Frostbite (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               enduro         ROMS/Enduro (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pacman        ROMS/Pac-Man (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          double_dunk ROMS/Double Dunk (Super Basketball) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               tennis ROMS/Tennis - Le Tennis (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          word_zapper ROMS/Word Zapper (Word Grabber) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           earthworld ROMS/SwordQuest - EarthWorld (Adventure I, SwordQuest I - EarthWorld) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             breakout ROMS/Breakout - Breakaway IV (Paddle) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             crossbow       ROMS/Crossbow (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       video_checkers ROMS/Video Checkers - Checkers - Atari Video Checkers (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      elevator_action ROMS/Elevator Action (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders ROMS/Space Invaders (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              othello        ROMS/Othello (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             air_raid ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             pitfall2 ROMS/Pitfall II - Lost Caverns (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           backgammon ROMS/Backgammon (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            king_kong      ROMS/King Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       tic_tac_toe_3d ROMS/3-D Tic-Tac-Toe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            atlantis2    ROMS/Atlantis II (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround - Chase (Blockade) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pooyan         ROMS/Pooyan (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              venture        ROMS/Venture (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              turmoil        ROMS/Turmoil (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         darkchambers ROMS/Dark Chambers (Dungeon, Dungeon Masters) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           videochess ROMS/Video Chess (Computer Chess) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         sir_lancelot   ROMS/Sir Lancelot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       kung_fu_master ROMS/Kung-Fu Master (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         yars_revenge ROMS/Yars' Revenge (Time Freeze) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             superman       ROMS/Superman (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               kaboom ROMS/Kaboom! (Paddle) (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      keystone_kapers ROMS/Keystone Kapers - Raueber und Gendarm (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          private_eye    ROMS/Private Eye (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             atlantis ROMS/Atlantis (Lost City of Atlantis) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               skiing ROMS/Skiing - Le Ski (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        crazy_climber  ROMS/Crazy Climber (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           ice_hockey ROMS/Ice Hockey - Le Hockey Sur Glace (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          star_gunner     ROMS/Stargunner (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 pong ROMS/Video Olympics - Pong Sports (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       name_this_game ROMS/Name This Game (Guardians of Treasure) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             carnival       ROMS/Carnival (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              bowling        ROMS/Bowling (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            tutankham      ROMS/Tutankham (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        wizard_of_wor  ROMS/Wizard of Wor (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Basic Math - Math (Math Pack) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           bank_heist ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        fishing_derby  ROMS/Fishing Derby (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            adventure      ROMS/Adventure (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               boxing ROMS/Boxing - La Boxe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         lost_luggage ROMS/Lost Luggage (Airport Mayhem) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             seaquest       ROMS/Seaquest (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               zaxxon         ROMS/Zaxxon (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               casino ROMS/Casino - Poker Plus (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       miniature_golf ROMS/Miniature Golf - Arcade Golf (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             trondead ROMS/TRON - Deadly Discs (TRON Joystick) (1983).bin\n",
            "\n",
            "\n",
            "\n",
            "Imported 110 / 110 ROMs\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.4)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.28.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: setuptools-scm>=4 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (6.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib) (59.5.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class features:\n",
        "    @staticmethod\n",
        "    def basic(frame, palette, background, crop_size=torch.Tensor([15,10])):\n",
        "        # For each color in palette, tell if each pixel is that color\n",
        "        # e.g. 4x4x3 image, with 2x3 palette, returns 4x4x2\n",
        "        colors_in_pixels = ((frame-background).unsqueeze(-2) == palette).all(-1)\n",
        "        # Split the image into `n_subimages`, each with dimension `crop_size`\n",
        "        frame_dims = torch.Tensor([*frame.shape[:2]])\n",
        "        n_subimages = (frame_dims/crop_size).prod().item()\n",
        "        if n_subimages.is_integer():\n",
        "            n_subimages = int(n_subimages)\n",
        "        else:\n",
        "            raise TypeError(f'n_subimages must be an integer, got `{n_subimages}` instead')\n",
        "        cropped_colors_in_pixels = colors_in_pixels.reshape((n_subimages, *crop_size.int().tolist(), colors_in_pixels.shape[-1]))\n",
        "        # Apply logical or insize each cropped image\n",
        "        cropped_features = cropped_colors_in_pixels.any(2).any(1)\n",
        "        # Flatten the features\n",
        "        features = cropped_features.flatten()\n",
        "        return features\n",
        "    \n",
        "    @staticmethod\n",
        "    def b_pros(frame, palette, crop_size=torch.Tensor([15,10])):\n",
        "        basic_features = features.basic(frame, palette, crop_size=crop_size)\n",
        "        pass"
      ],
      "metadata": {
        "id": "XkdKG3bVNwZv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvALE:\n",
        "    def __init__(self, rom, out_dir='ale-results', display=False, seed=0, feature_type='ScreenRGB',\n",
        "                 regen_bg=False, bg_samples=18000):\n",
        "        self.rom = rom\n",
        "        self.rom_name = rom.stem\n",
        "        self.out_dir = Path(out_dir).resolve()\n",
        "        self.out_dir.mkdir(exist_ok=True)\n",
        "        self.feature_type = feature_type\n",
        "\n",
        "        self.ale = ALEInterface()\n",
        "        self.ale.setInt(\"random_seed\", seed)\n",
        "        if display and SDL_SUPPORT and 'google.colab' not in sys.modules:\n",
        "            ale.setBool(\"sound\", True)\n",
        "            ale.setBool(\"display_screen\", True)\n",
        "        self.ale.loadROM(rom)\n",
        "\n",
        "        self.action_space = self.ale.getMinimalActionSet()\n",
        "        self.color_palette = self._get_color_palette().to(device)\n",
        "\n",
        "        self.bg_path = Path(f'./backgrounds/{self.rom_name}.pickle')\n",
        "        if regen_bg or not self.bg_path.exists() or not self.bg_path.is_file():\n",
        "            self.background = self._get_background(n_samples=bg_samples)\n",
        "        else:\n",
        "            with open(self.bg_path, 'rb') as file:\n",
        "                self.background = pickle.load(file).to(device)\n",
        "        \n",
        "        self._set_observe_method(feature_type)\n",
        "\n",
        "        # Default values\n",
        "        self._timestep = 0\n",
        "        self._do_record = False\n",
        "        self._record_padding = None\n",
        "\n",
        "    def reset(self, do_record=False):\n",
        "        self.ale.reset_game()\n",
        "        observation = self._observe()\n",
        "        self._timestep = 0\n",
        "\n",
        "        self._do_record = do_record\n",
        "        self._handle_recording()\n",
        "        \n",
        "        return observation\n",
        "        \n",
        "    def step(self, action):\n",
        "        if isinstance(action, int):\n",
        "            action = self.action_space[action]\n",
        "        reward = self.ale.act(action)\n",
        "        observation = self._observe()\n",
        "        done = self.ale.game_over()\n",
        "        self._timestep += 1\n",
        "        \n",
        "        self._handle_recording()\n",
        "        \n",
        "        return observation, reward, done, None\n",
        "\n",
        "    def show_video(self, scale=1):\n",
        "        \"\"\"Show a .mp4 video in html format of the recorded episode\"\"\"\n",
        "        filepath = self.out_dir.joinpath('record.mp4')\n",
        "        video_b64 = base64.b64encode(filepath.read_bytes())\n",
        "        html = f'''<video alt=\"{filepath}\" autoplay loop controls style=\"height:300px\">\n",
        "                        <source src=\"data:video/mp4;base64,{video_b64.decode('ascii')}\" type=\"video/mp4\" />\n",
        "                   </video>'''\n",
        "        ipythondisplay.display(ipythondisplay.HTML(data=html))\n",
        "\n",
        "    def _set_observe_method(self, feature_type):\n",
        "        if feature_type == 'ScreenRGB':\n",
        "            self._observe = lambda: torch.from_numpy(self.ale.getScreenRGB()).to(device)\n",
        "        elif feature_type == 'ScreenGrayscale':\n",
        "            self._observe = lambda: torch.from_numpy(self.ale.getScreenGrayscale()).to(device)\n",
        "        elif feature_type == 'Basic':\n",
        "            self._observe = lambda: features.basic(frame=torch.from_numpy(self.ale.getScreenRGB()).to(device),\n",
        "                                                   palette=self.color_palette,\n",
        "                                                   background=self.background)\n",
        "        else:\n",
        "            raise NotImplementedError(f'Feature type `{feature_type}` is not supported')\n",
        "        \n",
        "    def _observe(self):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def _get_color_palette(self):\n",
        "        result = subprocess.run(['python', '-c', f'__import__(\"ale_py\").ALEInterface().loadROM(\"{str(self.rom)}\")'], capture_output=True)\n",
        "        palette_name = result.stderr.decode().splitlines()[6].strip().split()[-1]\n",
        "        with open(f'palettes/{palette_name}_Palette.pickle', 'rb') as file:\n",
        "            palette = pickle.load(file)\n",
        "        return palette\n",
        "    \n",
        "    def _get_background(self, n_samples):\n",
        "        bg_feature_type = 'ScreenRGB' if self.feature_type not in ['ScreenGrayscale',] else 'ScreenGrayscale'\n",
        "        self._set_observe_method(bg_feature_type)\n",
        "        \n",
        "        sample_i = 0\n",
        "        pixel_histogram = torch.zeros((*self.ale.getScreenDims(), self.color_palette.shape[0]), dtype=int).to(device)\n",
        "        while sample_i < n_samples:\n",
        "            done, observation = False, self.reset()\n",
        "            while not done and sample_i < n_samples:\n",
        "                if not sample_i%10:\n",
        "                    print(f'\\rGenerating background... {sample_i}/{n_samples} samples ({sample_i/n_samples:.0%})', end='')\n",
        "                action = random.choice(self.action_space)\n",
        "                observation, reward, done, info = self.step(action)\n",
        "                observation = torch.from_numpy(observation).to(device)\n",
        "                colors_in_pixels = (observation.unsqueeze(-2) == self.color_palette).all(-1)\n",
        "                # for i in range(colors_in_pixels.shape[-1]):\n",
        "                #     print(colors_in_pixels.reshape(-1, 128))\n",
        "                pixel_histogram += colors_in_pixels\n",
        "                sample_i += 1\n",
        "        background_ids = pixel_histogram.argmax(axis=-1)\n",
        "        background = self.color_palette[background_ids]\n",
        "        \n",
        "        self.bg_path.parent.mkdir(exist_ok=True)\n",
        "        with open(self.bg_path, 'wb') as file:\n",
        "            pickle.dump(background.cpu(), file)\n",
        "        \n",
        "        return background\n",
        "    \n",
        "    def _handle_recording(self):\n",
        "        # Do nothing if not asked to record\n",
        "        if not self._do_record: return\n",
        "        # This is a new episode, delete previously recorded steps\n",
        "        if not self._timestep:\n",
        "            self.out_dir.joinpath('record').mkdir(exist_ok=True)\n",
        "            for step_png in self.out_dir.glob('record/step_*.png'):\n",
        "                step_png.unlink()\n",
        "            self._record_padding = None\n",
        "        # Record current timestep png\n",
        "        out_path = self.out_dir.joinpath(f'record/step_{self._timestep}.png')\n",
        "        self.ale.saveScreenPNG(str(out_path))\n",
        "        # Once the episode is over, format all png filenames to have the same integer 0 padding\n",
        "        if self.ale.game_over():\n",
        "            self._record_padding = len(str(self._timestep))\n",
        "            self._standardize_record_padding()\n",
        "            self._png_to_mp4()\n",
        "    \n",
        "    def _standardize_record_padding(self):\n",
        "        number_pattern = re.compile('\\d+')\n",
        "        for png in self.out_dir.glob('record/step_*.png'):\n",
        "            timestep = int(number_pattern.search(png.stem).group(0))\n",
        "            new_name = png.parent.joinpath(f'step_{timestep:0{self._record_padding}d}.png')\n",
        "            png.rename(new_name)\n",
        "\n",
        "    def _png_to_mp4(self):\n",
        "        \"\"\"Convert the recorded set of png files into a mp4 video\"\"\"\n",
        "        in_dir = self.out_dir.joinpath('record')\n",
        "        in_pattern = self.out_dir.joinpath(f'record/step_%0{self._record_padding}d.png')\n",
        "        out_file = self.out_dir.joinpath('record.mp4')\n",
        "        !cd $in_dir; ffmpeg -hide_banner -loglevel error -r 60 -i $in_pattern -vcodec libx264 -crf 25 -pix_fmt yuv420p -y $out_file\n"
      ],
      "metadata": {
        "id": "tUMUVMCwNxkI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sarsa phi eb"
      ],
      "metadata": {
        "id": "T3DHkXGnN0pt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqiqcC4RVWq7"
      },
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2017-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# all possible actions\n",
        "ACTIONS = range(4)\n",
        "\n",
        "# discount is always 1.0 in these experiments\n",
        "DISCOUNT = 1.0\n",
        "\n",
        "# use optimistic initial value, so it's ok to set epsilon to 0\n",
        "EPSILON = 0.05\n",
        "\n",
        "# maximum steps per episode\n",
        "STEP_LIMIT = 5000\n",
        "\n",
        "\n",
        "# get action at @position and @velocity based on epsilon greedy policy and @valueFunction  #########################    use our own get_action. modified it, may work as intended\n",
        "def get_action(observation, valueFunction):\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "        return np.random.choice(ACTIONS)\n",
        "    values = []\n",
        "    for action in ACTIONS:\n",
        "        values.append(valueFunction.value(observation))  \n",
        "    return np.argmax(values)\n",
        "\n",
        "\n",
        "\n",
        "# replacing trace update rule\n",
        "# @trace: old trace (will be modified)\n",
        "# @activeTiles: current active tile indices\n",
        "# @lam: lambda\n",
        "# @return: new trace for convenience\n",
        "def replacing_trace(trace, activeTiles, lam):\n",
        "    active = np.in1d(np.arange(len(trace)), activeTiles)\n",
        "    trace[active] = 1\n",
        "    trace[~active] *= lam * DISCOUNT\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "# wrapper class for Sarsa(lambda)\n",
        "class Sarsa:\n",
        "    # In this example I use the tiling software instead of implementing standard tiling by myself\n",
        "    # One important thing is that tiling is only a map from (state, action) to a series of indices\n",
        "    # It doesn't matter whether the indices have meaning, only if this map satisfy some property\n",
        "    # View the following webpage for more information\n",
        "    # http://incompleteideas.net/sutton/tiles/tiles3.html\n",
        "    # @maxSize: the maximum # of indices\n",
        "    #the hashing is a lfa?\n",
        "    def __init__(self, step_size, lam, trace_update=replacing_trace, max_size=28672):\n",
        "        self.max_size = max_size\n",
        "        self.trace_update = trace_update\n",
        "        self.lam = lam\n",
        "\n",
        "        # divide step size equally to each tiling\n",
        "        self.step_size = step_size/10\n",
        "\n",
        "        # weight for each tile\n",
        "        self.weights = np.zeros(max_size) #max size is the number of features?\n",
        "\n",
        "        # trace for each tile\n",
        "        self.trace = np.zeros(max_size)\n",
        "\n",
        "\n",
        "\n",
        "    # estimate the value of given state and action\n",
        "    def value(self, observation):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        return np.sum(self.weights[active_tiles])\n",
        "\n",
        "    # learn with given state, action and target\n",
        "    def learn(self, observation, target):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        estimation = np.sum(self.weights[active_tiles])\n",
        "        delta = target - estimation\n",
        "        print('target: ' + str(target))\n",
        "        #print('estimation array: ' + str(self.weights[active_tiles]))\n",
        "        print('estimation: ' + str(np.sum(self.weights[active_tiles])))\n",
        "        if self.trace_update == replacing_trace:\n",
        "            self.trace_update(self.trace, active_tiles, self.lam)\n",
        "        else:\n",
        "            raise Exception('Unexpected Trace Type')\n",
        "        self.weights += self.step_size * delta * self.trace\n",
        "        print('delta: ' + str(delta))\n",
        "        print('traces: ' + str(self.trace))\n",
        "        print('weights: ' +  str(self.weights))\n",
        "\n",
        "\n",
        "# play Mountain Car for one episode based on given method @evaluator\n",
        "# @return: total steps in this episode\n",
        "def play(evaluator, env):\n",
        "\n",
        "    action = random.choice(ACTIONS)\n",
        "    steps = 0\n",
        "    while True:\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        next_action = get_action(next_observation, evaluator)    #########################    use our own get_action  ??? modified it, may work as intented\n",
        "        steps += 1\n",
        "        target = reward + DISCOUNT * evaluator.value(next_observation)          ############# use our own value function ??? modified it, may work as intented\n",
        "        evaluator.learn(observation, target)\n",
        "        observation = next_observation\n",
        "        action = next_action\n",
        "        if done:\n",
        "            break\n",
        "        if steps >= STEP_LIMIT:\n",
        "            print('Step Limit Exceeded!')\n",
        "            break\n",
        "    return steps"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMeWJRRueo1t"
      },
      "source": [
        "class BaseAgent:\n",
        "  \"\"\" The base agent class function.\n",
        "  \"\"\"\n",
        "  def __init__(self, nb_features=28672):\n",
        "    #nothing for now\n",
        "    self.gamma = 1\n",
        "    self.features = nb_features\n",
        "    self.rhos = np.ones(self.features) #stores the rho_i values\n",
        "\n",
        "\n",
        "  def takeAction(self, t):\n",
        "    phis = [[0,1,0],[0,1,0],[0,1,0],[1,0,1]]\n",
        "    return phis[t]\n",
        "\n",
        "\n",
        "  def updateRho_i(self, counts, t):\n",
        "    M = self.features\n",
        "    self.rhos = (counts+1.5)/(t+1)\n",
        "    print('rhos: ' + str(self.rhos))\n",
        "    return 0\n",
        "\n",
        "\n",
        "  def PHI_EB(self, evaluator, env, beta=0.05, t_end=200):\n",
        "    t = 0\n",
        "    M = self.features #number of features\n",
        "    counts = np.zeros(M)\n",
        "    states = np.zeros((t_end,M)) #stores the previous phis for all timesteps\n",
        "\n",
        "    action = 1 #starting the game for the agent on the first game\n",
        "    old_phi = env._observe()\n",
        "    print('starting iterations')\n",
        "    print('rhos: ' + str(self.rhos))\n",
        "    while t < t_end:\n",
        "      #observe phi(s), reward\n",
        "      phi, reward, done, info = env.step(action)\n",
        "      next_action = get_action(phi, evaluator)\n",
        "      print('phi: ' + str(phi))\n",
        "      \n",
        "      #compute rho_t(phi) (feature visit-density)\n",
        "      if t > 0:\n",
        "        counts = np.sum(np.where(phi == states[0:t],1,0),axis=0)\n",
        "        print(counts)\n",
        "        self.rhos = (counts+0.5)/(t+1)\n",
        "        print('rhos: ' + str(self.rhos))\n",
        "        rho_t = np.prod(self.rhos)\n",
        "      else:\n",
        "        rho_t = 0.5**M\n",
        "      print('rho_t ' + str(rho_t))\n",
        "      #update all rho_i with observed phi\n",
        "      states[t] = phi\n",
        "      self.updateRho_i(counts, t+1)\n",
        "      print('min rho_i_t: ' + str(min(self.rhos)))\n",
        "      \n",
        "      #compute rho_t+1(phi)\n",
        "      new_rho_t = 1\n",
        "      for i in range(M):\n",
        "        new_rho_t = new_rho_t*self.rhos[i]\n",
        "      if new_rho_t <= 1e-323: #this is to avoid division by zero, might need to be tweaked\n",
        "        new_rho_t = 1e-323\n",
        "      print('new_rho_t ' + str(new_rho_t))\n",
        "\n",
        "      #compute Nhat_t(s)\n",
        "      Nhat_t = rho_t*(1-new_rho_t)/(new_rho_t-rho_t)\n",
        "      if Nhat_t <= 1e-323: #this is to avoid division by zero again, might need to be tweaked\n",
        "        Nhat_t = 1e-323\n",
        "\n",
        "      #compute R(s,a) (empirical reward)\n",
        "      explorationBonus = beta/np.sqrt(Nhat_t)\n",
        "\n",
        "      print(explorationBonus)\n",
        "      reward = reward + explorationBonus\n",
        "\n",
        "      print('weights: ' + str(evaluator.weights))\n",
        "      print(max(evaluator.weights))\n",
        "      print('state value: ' + str(evaluator.value(phi)))\n",
        "      #pass phi(s) and reward to RL algo to update theta_t\n",
        "      target = reward + self.gamma * evaluator.value(phi)          ############# use our own value function ??? modified it, may work as intented\n",
        "      evaluator.learn(old_phi, target)\n",
        "\n",
        "      if done:\n",
        "        #break\n",
        "        env.reset()\n",
        "        action = 1\n",
        "        old_phi = env.observe()\n",
        "      else:\n",
        "        old_phi = phi\n",
        "        action = next_action\n",
        "\n",
        "      t += 1\n",
        "\n",
        "    return evaluator.weights\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6Wzc-9sLqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "3e864612-9edd-4bb2-d65e-ee0433947b3a"
      },
      "source": [
        "from ale_py.roms import Breakout\n",
        "env = EnvALE(Breakout, feature_type='Basic')\n",
        "print(env.action_space)\n",
        "alpha = 0.25\n",
        "lam = 0.99\n",
        "evaluator = Sarsa(alpha, lam, replacing_trace,28672)\n",
        "agent = BaseAgent()\n",
        "env.reset(do_record=True)\n",
        "weights = agent.PHI_EB(evaluator, env, beta=0.05, t_end=100)\n",
        "env.show_video()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Action.NOOP: 0>, <Action.FIRE: 1>, <Action.RIGHT: 3>, <Action.LEFT: 4>]\n",
            "starting iterations\n",
            "rhos: [1. 1. 1. ... 1. 1. 1.]\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rho_t 0.0\n",
            "rhos: [0.75 0.75 0.75 ... 0.75 0.75 0.75]\n",
            "min rho_i_t: 0.75\n",
            "new_rho_t 1e-323\n",
            "1.590606226047598e+160\n",
            "weights: [0. 0. 0. ... 0. 0. 0.]\n",
            "0.0\n",
            "state value: 0.0\n",
            "target: 1.590606226047598e+160\n",
            "estimation: 0.0\n",
            "delta: 1.590606226047598e+160\n",
            "traces: [1. 0. 0. ... 0. 0. 0.]\n",
            "weights: [3.97651557e+158 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
            " 0.00000000e+000 0.00000000e+000]\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "0\n",
            "rhos: 0.25\n",
            "rho_t 0.25\n",
            "rhos: 0.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c3148870061b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPHI_EB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3bd9ffd09e36>\u001b[0m in \u001b[0;36mPHI_EB\u001b[0;34m(self, evaluator, env, beta, t_end)\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateRho_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min rho_i_t: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;31m#compute rho_t+1(phi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nW2-6Cf2WNFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimisation tests\n",
        "%%timeit\n",
        "t = 1\n",
        "M = 28000\n",
        "counts = np.zeros(M)\n",
        "rhos = np.ones(M)\n",
        "for i in range(M): #M is the number of features of phi\n",
        "  counts[i] += 1 #since we add phi to the seen states, all the counts are increased by one for t+1\n",
        "  rhos[i] = (counts[i]+0.5)/(t+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cKj8q7PjAT-",
        "outputId": "4bac8aad-a70b-48b4-fee9-d89d8e4e7695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 36.7 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best one\n",
        "%%timeit\n",
        "t = 1\n",
        "M = 28000\n",
        "counts = np.zeros(M)\n",
        "rhos = np.ones(M)\n",
        "rhos = (counts+1.5)/(t+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wphENKYrjSD2",
        "outputId": "a7c3af0e-98d1-4a0b-b43f-4da581559846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 12.92 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 54.5 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bPrAblBmqawT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute rho_t(phi) (feature visit-density)\n",
        "%%timeit\n",
        "t = 0\n",
        "t_end = 30\n",
        "M = 10000\n",
        "phis = np.zeros((t_end,M))\n",
        "counts = np.zeros(M)\n",
        "states = np.zeros((t_end,M))\n",
        "rhos = np.ones(M)\n",
        "while t < t_end:\n",
        "  phi = phis[t]\n",
        "  if t > 0:\n",
        "    rho_t = 1\n",
        "    for i in range(M):\n",
        "      counts[i] = 0\n",
        "      for step in range(t):\n",
        "        if phi[i] == states[step,i]:\n",
        "          counts[i] += 1\n",
        "      rhos[i] = (counts[i]+0.5)/(t+1)\n",
        "      rho_t = rho_t*rhos[i]\n",
        "  else:\n",
        "    rho_t = 0.5**M\n",
        "\n",
        "  #updating rho (other method)\n",
        "  states[t] = phi\n",
        "  counts += 1\n",
        "  rhos = (counts+0.5)/(t+2)\n",
        "  \n",
        "  t += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqH7kGYDk2Pr",
        "outputId": "da68666f-f6e3-4c21-e58a-190a6f404168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 4.51 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute rho_t(phi) (feature visit-density)\n",
        "%%timeit\n",
        "t = 0\n",
        "t_end = 5000\n",
        "M = 1000\n",
        "phis = np.zeros((t_end,M))\n",
        "counts = np.zeros(M)\n",
        "states = np.zeros((t_end,M))\n",
        "rhos = np.ones(M)\n",
        "while t < t_end:\n",
        "  phi = phis[t]\n",
        "  if t > 0:\n",
        "    rho_t = 1\n",
        "    counts = np.zeros(M)\n",
        "    for step in range(t):\n",
        "      counts[np.where(phi == states[step])] += 1\n",
        "    rhos = (counts+0.5)/(t+1)\n",
        "    rho_t = np.prod(rhos)\n",
        "  else:\n",
        "    rho_t = 0.5**M\n",
        "\n",
        "  #updating rho (other method)\n",
        "  states[t] = phi\n",
        "  counts += 1\n",
        "  rhos = (counts+0.5)/(t+2)\n",
        "  \n",
        "  t += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j82o_Tzqoi2K",
        "outputId": "248d9a7b-1aa9-45a2-87a2-a8770cbd366d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2min 19s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#version 3 of optimisation\n",
        "%%timeit\n",
        "t = 0\n",
        "t_end = 5000\n",
        "M = 1000\n",
        "#phis = [[0,1,0],[0,1,0],[0,1,0],[1,1,0]]\n",
        "phis = np.zeros((t_end,M))\n",
        "counts = np.zeros(M)\n",
        "states = np.zeros((t_end,M))\n",
        "rhos = np.ones(M)\n",
        "while t < t_end:\n",
        "  phi = phis[t]\n",
        "  if t > 0:\n",
        "    rho_t = 1\n",
        "    counts = np.sum(np.where(phi == states[0:t],1,0),axis=0)\n",
        "    rhos = (counts+0.5)/(t+1)\n",
        "    rho_t = np.prod(rhos)\n",
        "  else:\n",
        "    rho_t = 0.5**M\n",
        "\n",
        "  #updating rho (other method)\n",
        "  states[t] = phi\n",
        "  counts += 1\n",
        "  rhos = (counts+0.5)/(t+2)\n",
        "  \n",
        "  t += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1irC6C3no0gV",
        "outputId": "c6384ce7-3844-418d-f8d6-7c237b7ab6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 45.1 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(phi)\n",
        "print(states)\n",
        "print(np.sum(np.where(phi == states,1,0),axis=0))\n",
        "print(counts[phi == states[3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ijq6sMt-Qd",
        "outputId": "6e939141-74fc-4777-abcc-e0dc4efc7f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 1. 0.]]\n",
            "[1 4 4]\n",
            "[0. 0. 0.]\n"
          ]
        }
      ]
    }
  ]
}